import os
import sys
import argparse
from datetime import datetime
from pathlib import Path
import torch
import random
import logging
import numpy as np
import pandas as pd
import multiprocessing


def polar_engine():
    """Return appropriate Polars engine based on GPU availability."""
    return "gpu" if torch.cuda.is_available() else "rust"


# ---- One base for everything ----
BASE_LOGGER = "base_logger"
_BASE = logging.getLogger(BASE_LOGGER)  # the only logger we configure here


def setup_logging(
    log_path: str | Path | None, level: str = "INFO"
) -> logging.Logger:
    """Configure the base logger once (file + console)."""
    if getattr(_BASE, "_configured", False):
        return _BASE

    _BASE.handlers.clear()
    _BASE.setLevel(getattr(logging, level.upper(), logging.INFO))
    fmt = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - pid=%(process)d - %(filename)s:%(lineno)d - %(funcName)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Optional file handler
    if log_path:
        fh = logging.FileHandler(str(log_path), encoding="utf-8")
        fh.setFormatter(fmt)
        _BASE.addHandler(fh)

    # Console handler
    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    _BASE.addHandler(sh)

    # Do not bubble to the *root* logger
    _BASE.propagate = False
    _BASE._configured = True
    return _BASE


def get_logger(name: str | None = None) -> logging.Logger:
    """Get a child logger that inherits the base handlers."""
    return logging.getLogger(
        BASE_LOGGER if not name else f"{BASE_LOGGER}.{name}"
    )


logger = get_logger(__name__)


def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in ("yes", "true", "t", "1"):
        return True
    elif v.lower() in ("no", "false", "f", "0"):
        return False
    else:
        raise argparse.ArgumentTypeError(
            "Expected a boolean value (true/false)"
        )


def set_seed(seed: int = 42):
    """Set seed for reproducibility across random, numpy, and torch."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # for multi-GPU

    # Ensure deterministic behavior
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # For full reproducibility in future versions
    os.environ["PYTHONHASHSEED"] = str(seed)


def save_csv_or_parquet(df: pd.DataFrame, fn: Path | None) -> None:
    if fn is None:
        logger.warning("No output file specified. Not saving.")
        return
    logger.info(f"Saving df to {fn}")
    if fn.suffix == ".parquet":
        df.to_parquet(fn)
    else:
        df.to_csv(fn, index=False)


def read_csv_or_parquet(fn: Path) -> pd.DataFrame:
    logger.info(f"Loading df from {fn}")
    if fn.suffix == ".parquet":
        return pd.read_parquet(fn)
    else:
        return pd.read_csv(fn)


def get_n_jobs(n_jobs: int) -> int:
    # Determine number of processes
    if n_jobs == -1:
        n_jobs = multiprocessing.cpu_count()
    elif n_jobs <= 1:
        n_jobs = 1
    return n_jobs


def parse_range(arg: str):
    """
    Parse a CLI argument that can be either:
      - a colon-separated range 'START:END' (inclusive of START, exclusive of END),
      - or a comma-separated list 'a,b,c'.
    Returns a Python range or list of numbers (int or float).
    """
    # Handle empty string
    if not arg or arg.strip() == "":
        return None

    if ":" in arg:
        start, end = arg.split(":")
        # Try float first, fall back to int
        try:
            start_val = float(start)
            end_val = float(end)
            if start_val.is_integer() and end_val.is_integer():
                return range(int(start_val), int(end_val))
            else:
                # For float ranges, we can't use range(), return a list
                import numpy as np

                return np.arange(start_val, end_val, 0.001).tolist()
        except ValueError:
            return range(int(start), int(end))
    elif "," in arg:
        # Try to parse as floats first, fall back to ints
        try:
            return [float(x.strip()) for x in arg.split(",")]
        except ValueError:
            return [int(x.strip()) for x in arg.split(",")]
    else:
        # single value - try float first, fall back to int
        try:
            val = float(arg)
            return [val]
        except ValueError:
            return [int(arg)]


def safe_autocorr(x: pd.Series, lag: int) -> float:
    """
    compute the lag-lag autocorrelation (i.e.,
    Pearson correlation between x_t and x_{tâˆ’lag})
    while being robust to NaNs and tiny samples.
    """
    x = pd.to_numeric(x, errors="coerce").astype(float)
    if x.isna().sum() > len(x) - 3 or len(x) <= lag + 2:
        return np.nan

    a = x.values
    b = np.roll(a, lag)
    b[:lag] = np.nan
    ok = ~np.isnan(a) & ~np.isnan(b)

    if ok.sum() < 3:
        return np.nan

    a_valid, b_valid = a[ok], b[ok]

    # Check for variation
    if len(np.unique(a_valid)) == 1 or len(np.unique(b_valid)) == 1:
        return np.nan

    a0 = a_valid - a_valid.mean()
    b0 = b_valid - b_valid.mean()

    a_std = np.sqrt(np.mean(a0**2))
    b_std = np.sqrt(np.mean(b0**2))

    if a_std == 0 or b_std == 0:
        return np.nan

    return (a0 * b0).mean() / (a_std * b_std)


def trend_slope(y: pd.Series) -> float:
    """
    Returns the correlation between value and time
    """
    y = pd.to_numeric(y, errors="coerce").astype(float)
    n = len(y)
    if n < 5 or y.notna().sum() < 5:
        return np.nan

    x = np.arange(n, dtype=float)
    ok = ~y.isna()
    x, y = x[ok], y[ok]

    if len(y) < 5 or len(np.unique(y)) == 1:
        return np.nan

    # Calculate correlation manually to avoid ddof issues
    x_mean, y_mean = x.mean(), y.mean()
    x_centered = x - x_mean
    y_centered = y - y_mean

    x_std = np.sqrt(np.mean(x_centered**2))
    y_std = np.sqrt(np.mean(y_centered**2))

    if x_std == 0 or y_std == 0:
        return np.nan

    return float(np.mean(x_centered * y_centered) / (x_std * y_std))


def seasonal_corr(values: pd.Series, period: int = 52) -> float:
    """Correlate with fundamental seasonal sine/cosine"""
    k = np.arange(len(values), dtype=float)
    s = np.sin(2 * np.pi * k / period)
    c = np.cos(2 * np.pi * k / period)
    v = pd.to_numeric(values, errors="coerce").astype(float).values
    ok = ~np.isnan(v)

    if ok.sum() < 8:
        return np.nan

    v_valid = v[ok]
    s_valid = s[ok]
    c_valid = c[ok]

    # Check for variation
    if len(np.unique(v_valid)) == 1:
        return np.nan

    # Manual standardization to avoid ddof issues
    v_mean = np.mean(v_valid)
    s_mean = np.mean(s_valid)
    c_mean = np.mean(c_valid)

    v_centered = v_valid - v_mean
    s_centered = s_valid - s_mean
    c_centered = c_valid - c_mean

    v_std = np.sqrt(np.mean(v_centered**2))
    s_std = np.sqrt(np.mean(s_centered**2))
    c_std = np.sqrt(np.mean(c_centered**2))

    if v_std == 0 or s_std == 0 or c_std == 0:
        return np.nan

    v_norm = v_centered / v_std
    s_norm = s_centered / s_std
    c_norm = c_centered / c_std

    cs = np.mean(v_norm * s_norm)
    cc = np.mean(v_norm * c_norm)
    return float(np.sqrt(cs**2 + cc**2))


# --- base aggregates with safer std calculation ---
def safe_std(s: pd.Series) -> float:
    """Calculate std safely without ddof warnings"""
    s_numeric = pd.to_numeric(s, errors="coerce")
    s_clean = s_numeric.dropna()
    if len(s_clean) <= 1:
        return np.nan
    # Use population std to avoid ddof issues
    return float(np.std(s_clean))


def safe_iqr(s: pd.Series) -> float:
    """Calculate IQR safely"""
    s_numeric = pd.to_numeric(s, errors="coerce")
    s_clean = s_numeric.dropna()
    if len(s_clean) < 2:
        return np.nan
    return float(np.percentile(s_clean, 75) - np.percentile(s_clean, 25))


def safe_median(s: pd.Series) -> float:
    """Calculate median safely"""
    s_numeric = pd.to_numeric(s, errors="coerce")
    s_clean = s_numeric.dropna()
    if len(s_clean) == 0:
        return np.nan
    return float(np.median(s_clean))


def safe_nanmean(s: pd.Series) -> float:
    """Calculate mean safely"""
    s_numeric = pd.to_numeric(s, errors="coerce")
    if s_numeric.isna().all():
        return np.nan
    return float(np.nanmean(s_numeric))
